{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JAMrV0wCdE5",
        "outputId": "138e1cea-97df-4874-845b-0972631a304f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Level  Uniq% (k=1)    Avg k  Prosecutor risk (mean 1/k)  Rows used\n",
            "Original        43.93 127.4438                      0.4564      10000\n",
            "    Mild        96.46   1.1230                      0.9748      10000\n",
            "Moderate        17.91  17.7002                      0.3427      10000\n",
            "  Strong        20.19   9.8338                      0.3766      10000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "# Put your file paths here:\n",
        "FILES = {\n",
        "    \"Original\":  \"OroginalDatasetBalanced.csv\",   # or .tsv\n",
        "    \"Mild\":      \"Obfuscated_Mild.csv\",\n",
        "    \"Moderate\":  \"/content/modarate_new.csv\",\n",
        "    \"Strong\":    \"Obfuscated_Strong.csv\",\n",
        "}\n",
        "\n",
        "# If your files are TSV (tab-separated), set SEP = \"\\t\"\n",
        "# If they are CSV (comma-separated), set SEP = \",\"\n",
        "SEP = \",\"  # change to \"\\t\" if needed\n",
        "\n",
        "# Choose quasi-identifiers (Q) that exist in ALL datasets\n",
        "# (avoid Flow ID / Src IP / Dst IP / Timestamp)\n",
        "QI_COLS = [\n",
        "    \"Protocol\",\n",
        "    \"Dst Port\",\n",
        "    \"Flow Duration\",\n",
        "    \"Total Fwd Packet\",\n",
        "    \"Total Bwd packets\",\n",
        "    \"Flow Bytes/s\",\n",
        "    \"Flow Packets/s\",\n",
        "]\n",
        "\n",
        "# Numeric QI columns to discretize into bins (consistent edges for all levels)\n",
        "NUM_QI = [\n",
        "    \"Flow Duration\",\n",
        "    \"Total Fwd Packet\",\n",
        "    \"Total Bwd packets\",\n",
        "    \"Flow Bytes/s\",\n",
        "    \"Flow Packets/s\",\n",
        "]\n",
        "\n",
        "N_BINS = 5  # keep consistent with your \"qcut (5 bins)\" idea\n",
        "# ---------------------------\n",
        "\n",
        "def read_table(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        sep=SEP,\n",
        "        engine=\"python\",\n",
        "        na_values=[\"\", \" \", \"NA\", \"NaN\"],\n",
        "    )\n",
        "    # Trim whitespace in column names\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def coerce_numeric(df: pd.DataFrame, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def make_bin_edges_from_reference(ref: pd.DataFrame, col: str, n_bins: int):\n",
        "    # Use quantiles from Original (reference) to build stable bin edges\n",
        "    x = ref[col].dropna().astype(float)\n",
        "    if x.empty:\n",
        "        return None\n",
        "    qs = np.linspace(0, 1, n_bins + 1)\n",
        "    edges = np.unique(np.quantile(x, qs))\n",
        "    if len(edges) < 3:\n",
        "        return None\n",
        "    edges[0] = -np.inf\n",
        "    edges[-1] = np.inf\n",
        "    return edges\n",
        "\n",
        "def apply_binning(df: pd.DataFrame, col: str, edges):\n",
        "    if edges is None or col not in df.columns:\n",
        "        return df\n",
        "    # If already looks like 0..4 bins, keep it\n",
        "    s = df[col]\n",
        "    if pd.api.types.is_integer_dtype(s) or pd.api.types.is_float_dtype(s):\n",
        "        vals = s.dropna().unique()\n",
        "        if len(vals) > 0 and np.all(np.isin(vals, [0,1,2,3,4])):\n",
        "            df[col] = s.astype(\"Int64\")\n",
        "            return df\n",
        "    df[col] = pd.cut(df[col].astype(float), bins=edges, labels=False, include_lowest=True).astype(\"Int64\")\n",
        "    return df\n",
        "\n",
        "def compute_k_metrics(df: pd.DataFrame, qi_cols):\n",
        "    d = df[qi_cols].copy()\n",
        "\n",
        "    # Drop rows where any QI is missing (keeps metric clean)\n",
        "    d = d.dropna()\n",
        "\n",
        "    # Equivalence class size k for each row\n",
        "    grp = d.groupby(qi_cols, dropna=False).size()\n",
        "    k = d.merge(grp.rename(\"k\"), left_on=qi_cols, right_index=True)[\"k\"].to_numpy()\n",
        "\n",
        "    uniq_pct = (k == 1).mean() * 100.0\n",
        "    avg_k = float(np.mean(k))\n",
        "    prosecutor = float(np.mean(1.0 / k))\n",
        "\n",
        "    return uniq_pct, avg_k, prosecutor, len(d)\n",
        "\n",
        "def main():\n",
        "    # Load all\n",
        "    data = {name: read_table(path) for name, path in FILES.items()}\n",
        "\n",
        "    # Basic checks\n",
        "    for name, df in data.items():\n",
        "        missing = [c for c in QI_COLS if c not in df.columns]\n",
        "        if missing:\n",
        "            raise ValueError(f\"{name} is missing columns: {missing}\")\n",
        "\n",
        "    # Coerce numeric\n",
        "    for name in data:\n",
        "        data[name] = coerce_numeric(data[name], NUM_QI)\n",
        "\n",
        "    # Build bin edges from ORIGINAL, apply to all levels (consistency!)\n",
        "    ref = data[\"Original\"]\n",
        "    edges_map = {c: make_bin_edges_from_reference(ref, c, N_BINS) for c in NUM_QI}\n",
        "\n",
        "    for name, df in data.items():\n",
        "        for c in NUM_QI:\n",
        "            df = apply_binning(df, c, edges_map[c])\n",
        "        data[name] = df\n",
        "\n",
        "    # Compute metrics\n",
        "    rows = []\n",
        "    for name, df in data.items():\n",
        "        uniq_pct, avg_k, prosecutor, n_used = compute_k_metrics(df, QI_COLS)\n",
        "        rows.append([name, uniq_pct, avg_k, prosecutor, n_used])\n",
        "\n",
        "    out = pd.DataFrame(rows, columns=[\"Level\", \"Uniq% (k=1)\", \"Avg k\", \"Prosecutor risk (mean 1/k)\", \"Rows used\"])\n",
        "    out = out.sort_values(\"Level\", key=lambda s: s.map({\"Original\":0,\"Mild\":1,\"Moderate\":2,\"Strong\":3}))\n",
        "    print(out.to_string(index=False))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}